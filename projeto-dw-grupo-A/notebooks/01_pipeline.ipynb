{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84c9396a-7814-489a-82d5-b4da35d84bf9",
   "metadata": {},
   "source": [
    "**Carrega as bibliotecas e testa a conex√£o com banco de dados PostGreeSQL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6399ae12-bd28-4add-aab0-19681d7711b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Sucesso: Conex√£o estabelecida com o banco de dados.\n"
     ]
    }
   ],
   "source": [
    "#Bibliotecas Python\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "#Bibliotecas Proprias\n",
    "if os.path.abspath(os.path.join(os.getcwd(), '..')) not in sys.path: sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "import src.connection as cn  # banco PostGreSQL\n",
    "\n",
    "#Teste de conexao\n",
    "cn.connection_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82929c6-464d-48c0-afaf-8405ec1b26e9",
   "metadata": {},
   "source": [
    "**Pipeline ETL/ELT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e398e3e4-cd1e-444f-ad22-c9a50b0821bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define path dos scripts SQL\n",
    "scripts_sql_path = os.path.abspath(os.path.join(os.getcwd(), '..')) + '\\\\scripts\\\\'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08da9972-6d9a-429d-8eca-3cae80c08db4",
   "metadata": {},
   "source": [
    "---\n",
    "**1. STAGING (00_staging.sql)**\n",
    "\n",
    "O script `00_staging.sql` √© respons√°vel pela **etapa de ingest√£o e prepara√ß√£o inicial (Staging)** dos dados no banco de dados. Ele cria tabelas tempor√°rias (`tmp_orders`, `tmp_items`, `tmp_customers`, `tmp_products` e `tmp_sellers`) que espelham a estrutura dos arquivos CSV originais do Olist, definindo os tipos de dados apropriados (como `TIMESTAMP` para datas e `NUMERIC` para valores monet√°rios). O script segue um padr√£o de limpeza (`DROP TABLE IF EXISTS`) seguido de cria√ß√£o (`CREATE TABLE`) e, finalmente, executa a carga em massa dos dados brutos utilizando o comando `COPY`, deixando o ambiente pronto para as transforma√ß√µes posteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9e423ea-7b33-4c7c-8e5e-a59335c99350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processando script: 00_staging.sql ---\n",
      "--- Iniciando execu√ß√£o de 16 bloco(s) ---\n",
      "Executando comando 3 (COPY especial)...\n",
      "  -> Carga de '../data/olist_orders_dataset.csv' em 'analytics.tmp_orders' conclu√≠da.\n",
      "Executando comando 6 (COPY especial)...\n",
      "  -> Carga de '../data/olist_order_items_dataset.csv' em 'analytics.tmp_items' conclu√≠da.\n",
      "Executando comando 9 (COPY especial)...\n",
      "  -> Carga de '../data/olist_customers_dataset.csv' em 'analytics.tmp_customers' conclu√≠da.\n",
      "Executando comando 12 (COPY especial)...\n",
      "  -> Carga de '../data/olist_products_dataset.csv' em 'analytics.tmp_products' conclu√≠da.\n",
      "Executando comando 15 (COPY especial)...\n",
      "  -> Carga de '../data/olist_sellers_dataset.csv' em 'analytics.tmp_sellers' conclu√≠da.\n",
      "--- [SUCESSO] Script '00_staging.sql' finalizado com sucesso. ---\n"
     ]
    }
   ],
   "source": [
    "# Executa 00_staging.sql\n",
    "cn.execute_pipeline_script(scripts_sql_path + '00_staging.sql')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8a9018-13ca-46de-909b-24f9b2938115",
   "metadata": {},
   "source": [
    "---\n",
    "**2. OLTP (01_oltp.sql)**\n",
    "\n",
    "O script `01_oltp.sql` √© respons√°vel pela **normaliza√ß√£o e refinamento** dos dados brutos que est√£o na √°rea de Staging, transformando as tabelas tempor√°rias (`tmp_*`) em tabelas relacionais definitivas (`tb_*`) prontas para um ambiente transacional. Ele aplica regras de qualidade de dados essenciais, como a **remo√ß√£o de duplicatas** (via `DISTINCT ON`), o **tratamento de valores nulos** (usando `COALESCE` para definir valores padr√£o) e, principalmente, a imposi√ß√£o de **integridade referencial** atrav√©s da cria√ß√£o de Chaves Prim√°rias e Estrangeiras, garantindo que apenas registros consistentes (ex: itens de pedido vinculados a pedidos e produtos existentes) sejam persistidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a55a06e9-1926-4d5d-8412-fa5da1295a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processando script: 01_oltp.sql ---\n",
      "--- Iniciando execu√ß√£o de 17 bloco(s) ---\n",
      "--- [SUCESSO] Script '01_oltp.sql' finalizado com sucesso. ---\n"
     ]
    }
   ],
   "source": [
    "# Executa 01_oltp.sql\n",
    "cn.execute_pipeline_script(scripts_sql_path + '01_oltp.sql')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37182d76-a9e8-443b-ae8d-b6ce5088df14",
   "metadata": {},
   "source": [
    "---\n",
    "**3. DW ESTRUTURA (02_dw_model.sql)**\n",
    "\n",
    "O script `02_dw_model.sql` √© respons√°vel pela **modelagem e cria√ß√£o da estrutura do Data Warehouse** no schema dedicado `analytics`, implementando um modelo dimensional do tipo *Star Schema*. Ele define as tabelas de dimens√£o (`dim_customer`, `dim_product`, `dim_seller`, `dim_date`) para armazenar atributos descritivos e a tabela fato (`fact_sales`) para armazenar as m√©tricas de vendas, utilizando chaves substitutas auto-incrementais (`SERIAL`) e chaves estrangeiras para garantir a integridade referencial, tudo de forma conservadora atrav√©s de instru√ß√µes `IF NOT EXISTS` para preservar dados caso as estruturas j√° existam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc61f828-bc15-4155-9e9a-1dee903b33b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processando script: 02_dw_model.sql ---\n",
      "--- Iniciando execu√ß√£o de 6 bloco(s) ---\n",
      "--- [SUCESSO] Script '02_dw_model.sql' finalizado com sucesso. ---\n"
     ]
    }
   ],
   "source": [
    "# Executa 02_dw_model.sql\n",
    "cn.execute_pipeline_script(scripts_sql_path + '02_dw_model.sql')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134fd812-3092-4618-b144-1c6df5594520",
   "metadata": {},
   "source": [
    "---\n",
    "**4. ETL CARGA (03_etl_load.sql)**\n",
    "\n",
    "O script `03_etl_load.sql` executa a fase final de **Carga (Load)** do pipeline ETL, respons√°vel por popular as tabelas do Data Warehouse (schema `analytics`) a partir dos dados transacionais j√° normalizados. Ele automatiza a cria√ß√£o da linha do tempo na `dim_date` cobrindo todo o hist√≥rico de pedidos, realiza a carga incremental das dimens√µes de neg√≥cio (Clientes, Produtos e Vendedores) inserindo apenas registros in√©ditos, e consolida as m√©tricas na tabela fato `fact_sales`, transformando os IDs originais em Chaves Substitutas (*Surrogate Keys*) atrav√©s de jun√ß√µes com as dimens√µes, estruturando assim os dados no formato *Star Schema* para an√°lise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9e0e66b-fc49-453d-a64e-59c26ad2da5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processando script: 03_etl_load.sql ---\n",
      "--- Iniciando execu√ß√£o de 7 bloco(s) ---\n",
      "--- [SUCESSO] Script '03_etl_load.sql' finalizado com sucesso. ---\n"
     ]
    }
   ],
   "source": [
    "# Executa 03_etl_load.sql\n",
    "cn.execute_pipeline_script(scripts_sql_path + '03_etl_load.sql')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97cc78b-814c-48b7-a399-20007fd6e56f",
   "metadata": {},
   "source": [
    "---\n",
    "**5. VALIDA√á√ïES (04_validation.sql)**\n",
    "\n",
    "O script `04_validation.sql` atua como uma etapa de **auditoria e controle de qualidade** (Data Quality) p√≥s-carga, garantindo a confiabilidade do Data Warehouse. Ele executa testes comparativos entre a origem (OLTP) e o destino (DW) para verificar tr√™s pilares fundamentais: a **volumetria** (se a quantidade de registros foi preservada sem perdas), a **integridade referencial** (se todas as vendas na tabela fato est√£o corretamente ligadas a clientes, produtos e vendedores existentes, buscando por chaves nulas) e a **consist√™ncia financeira** (se o somat√≥rio dos valores monet√°rios permanece id√™ntico ap√≥s as transforma√ß√µes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6bb522c-c84a-4a31-9bea-3facdc6a8acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- üîç Iniciando Valida√ß√£o: 04_validation.sql ---\n",
      "\n",
      "üìä Consulta #1:\n",
      "                            tabela_origem  total_registros\n",
      "          3. DW: Fato Vendas (fact_sales)           112650\n",
      "             1. OLTP: Pedidos (tb_orders)            99441\n",
      "2. OLTP: Itens de Pedido (tb_order_items)           112650\n",
      "\n",
      "üìä Consulta #2:\n",
      "                     validacao  qtd_erros\n",
      "Registros com Customer_SK Nulo          0\n",
      "    Registros com Date_SK Nulo          0\n",
      " Registros com Product_SK Nulo          0\n",
      "  Registros com Seller_SK Nulo          0\n",
      "\n",
      "üìä Consulta #3:\n",
      "               metrica  valor_total\n",
      "  DW: Soma Total Pre√ßo   13591643.7\n",
      "OLTP: Soma Total Pre√ßo   13591643.7\n",
      "\n",
      "--- ‚úÖ Valida√ß√£o Conclu√≠da. 3 consultas executadas. ---\n"
     ]
    }
   ],
   "source": [
    "# Executa 04_validation.sql\n",
    "cn.execute_query_script(scripts_sql_path + '04_validation.sql')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f08218-7e49-42ab-a1e5-1c7c977bbc11",
   "metadata": {},
   "source": [
    "---\n",
    "**6. Limpeza (99_cleanup.sql)**\n",
    "\n",
    "O script `99_cleanup.sql` atua como uma rotina de **faxina final** do pipeline, respons√°vel por remover do banco de dados todos os artefatos intermedi√°rios que n√£o s√£o mais necess√°rios ap√≥s a carga do Data Warehouse. Ele executa a exclus√£o segura (usando `DROP TABLE IF EXISTS ... CASCADE`) tanto das tabelas transacionais normalizadas (`tb_*`) quanto das tabelas de *staging* (`analytics.tmp_*`), liberando espa√ßo de armazenamento e garantindo que o ambiente fique limpo para futuras execu√ß√µes, mantendo preservadas apenas as tabelas finais do DW (`analytics.dim_*` e `fact_*`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9482839a-818a-4c0f-81c1-1bb3e0334d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processando script: 99_cleanup.sql ---\n",
      "--- Iniciando execu√ß√£o de 10 bloco(s) ---\n",
      "--- [SUCESSO] Script '99_cleanup.sql' finalizado com sucesso. ---\n"
     ]
    }
   ],
   "source": [
    "# Executa 99_cleanup.sql\n",
    "cn.execute_pipeline_script(scripts_sql_path + '99_cleanup.sql')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95106788-9e5f-44b4-bd3c-0a90fd29c424",
   "metadata": {},
   "source": [
    "---\n",
    "**7. Visualiza√ß√£o tabelas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de986c69-262d-4a07-9d85-7fb76831c02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecionando 7 tabelas no schema 'analytics'...\n",
      "Arquivo 'tabelas_colunas.csv' salvo com sucesso.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tablename</th>\n",
       "      <th>record_count</th>\n",
       "      <th>columns_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>df_orders</td>\n",
       "      <td>99441</td>\n",
       "      <td>[order_id, customer_id, order_status, order_pu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dim_customer</td>\n",
       "      <td>99441</td>\n",
       "      <td>[customer_sk, customer_id, effective_from, eff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fact_sales</td>\n",
       "      <td>112650</td>\n",
       "      <td>[sale_sk, order_id, customer_sk, product_sk, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dim_product</td>\n",
       "      <td>32951</td>\n",
       "      <td>[product_sk, product_id]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dim_seller</td>\n",
       "      <td>3095</td>\n",
       "      <td>[seller_sk, seller_id]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dim_date</td>\n",
       "      <td>774</td>\n",
       "      <td>[date_sk, full_date, year, month, day, quarter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>df_items</td>\n",
       "      <td>112650</td>\n",
       "      <td>[order_id, order_item_id, product_id, seller_i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      tablename  record_count  \\\n",
       "0     df_orders         99441   \n",
       "1  dim_customer         99441   \n",
       "2    fact_sales        112650   \n",
       "3   dim_product         32951   \n",
       "4    dim_seller          3095   \n",
       "5      dim_date           774   \n",
       "6      df_items        112650   \n",
       "\n",
       "                                        columns_list  \n",
       "0  [order_id, customer_id, order_status, order_pu...  \n",
       "1  [customer_sk, customer_id, effective_from, eff...  \n",
       "2  [sale_sk, order_id, customer_sk, product_sk, s...  \n",
       "3                           [product_sk, product_id]  \n",
       "4                             [seller_sk, seller_id]  \n",
       "5  [date_sk, full_date, year, month, day, quarter...  \n",
       "6  [order_id, order_item_id, product_id, seller_i...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Retorna tabelas no banco de dados\n",
    "cn.get_schema_metadata(session=cn.get_session())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cda9c06-4b66-4b9d-a481-932d85c800dd",
   "metadata": {},
   "source": [
    "**8. Dump dos dados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b0d0732-59e0-4ccb-9ea3-b2de929368f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üêç Iniciando Backup Python (Pure SQL) para 'meu_backup_completo.sql' ---\n",
      "Lendo tabela: analytics.dim_customer...\n",
      "  -> 99441 registros escritos para 'dim_customer'.\n",
      "Lendo tabela: analytics.dim_product...\n",
      "  -> 32951 registros escritos para 'dim_product'.\n",
      "Lendo tabela: analytics.dim_seller...\n",
      "  -> 3095 registros escritos para 'dim_seller'.\n",
      "Lendo tabela: analytics.dim_date...\n",
      "  -> 774 registros escritos para 'dim_date'.\n",
      "Lendo tabela: analytics.fact_sales...\n",
      "  -> 112650 registros escritos para 'fact_sales'.\n",
      "‚úÖ Backup conclu√≠do! Arquivo salvo em: meu_backup_completo.sql\n"
     ]
    }
   ],
   "source": [
    "# Gera o arquivo .sql com os dados atuais\n",
    "cn.generate_python_backup(\"dump_data.sql\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
